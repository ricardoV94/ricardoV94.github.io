[{"id":0,"href":"/posts/gamma_distribution/","title":"A most unprincipled derivation of the gamma distribution","section":"Blog","content":"Introduction #  In this article I will derive the gamma distribution in a most unprincipled way.\nWhy? First, there are many good resources out there explaining how to derive the gamma distribution from first principles, usually involving these idealized things called poisson processes. These are great sources and I would definitely recommend them. This one by Aerin Kim is amazing.\nHowever, more often than not, people use gamma distributions in real world problems for much more mundane reasons: It\u0026rsquo;s a well behaved yet flexible positive continuous distribution.\nBy deriving a distribution from a purely utilitarian perspective, we might get a better intuition on how and when to use it. Besides, once you go through such exercise, continuous probability distributions may start to seem a bit less magical. They may even make sense!\n  \\(\\)  Continuous probability distributions are mathematical objects that can be fully characterized by a probability density function (pdf). This function computes the density probability of a random outcome \\(x\\) given a pre-specified set of parameters. It must respect two simple constraints:\n Always return a non-negative density for every possible parameter and random outcome \\(x\\). The density of every possible outcome \\(x\\), given a fixed set of parameters, must integrate to 1.  To compute the probability that an event \\(x\\) will fall in the range \\([a, b]\\), we integrate the pdf over that range. The second requirement is simply saying that if \\([a, b]\\) corresponds to the range of all possible values of \\(x\\), the probability obtained from integrating the pdf must be 1. This makes sense given the convention that an event with probability of 1 corresponds to an absolutely certain event.\nWarmup exercise: the uniform distribution #  To warmup, it may help to start with what is perhaps the simplest continuous probability distribution: the uniform. This core of this distribution is the constant function\n\\[ uniform(x; lower, upper) = 1\\]\nimport numpy as np import scipy.special import scipy.integrate import matplotlib.pyplot as plt import seaborn seaborn.set_style(\u0026#39;darkgrid\u0026#39;) seaborn.set(font_scale=1.4) def uniform(x, lower, upper): return np.ones_like(x) def plot_uniform(f, x, lower, upper, ax, color, pdf=False): y = f(x, lower, upper) area_y = scipy.integrate.quad(f, lower, upper, args=(lower, upper))[0] ax.plot(x, y, color=color) ax.fill_between(x, y, color=color, alpha=.2) ax.text( 0.5, 0.5, f\u0026#39;area={area_y:.1f}\u0026#39;, transform=ax.transAxes, ha=\u0026#39;center\u0026#39;, va=\u0026#39;center\u0026#39; ) ax.set_xticks([lower, upper]) ax.set_xlabel(\u0026#39;x\u0026#39;) if not pdf: ax.set_title(f\u0026#39;$uniform(x; {lower}, {upper})$\u0026#39;) else: ax.set_title(f\u0026#39;$uniform_{{pdf}}(x; {lower}, {upper})$\u0026#39;) _, ax = plt.subplots(1, 3, figsize=(14, 3.5)) lower, upper = 0, 1 x = np.linspace(lower, upper) plot_uniform(uniform, x, lower, upper, ax=ax[0], color=\u0026#39;C0\u0026#39;) lower, upper = 0.5, 2.5 x = np.linspace(lower, upper) plot_uniform(uniform, x, lower, upper, ax=ax[1], color=\u0026#39;C1\u0026#39;) lower, upper = 2, 2.5 x = np.linspace(lower, upper) plot_uniform(uniform, x, lower, upper, ax=ax[2], color=\u0026#39;C2\u0026#39;) plt.tight_layout() This conforms to the first requirement from above: all densities for possible values of \\(x\\) evaluate to a non-negative number (in this case 1.0). However, the second requirement is not fulfilled. To achieve this, all we need to do is to divide our original expression by the total area\n\\[ \\begin{aligned} uniform_{pdf}(x; lower, upper) \u0026amp;= \\frac{1}{\\int_{lower}^{upper} 1 dx} \\\\ \u0026amp;= \\frac{1}{x|_{lower}^{upper}} \\\\ \u0026amp;= \\frac{1}{upper - lower} \\\\ \\end{aligned} \\]\nAnd now we have a proper uniform probability density function! It might seem like we cheated, but this is really all there is to it.\ndef uniform_pdf(x, lower, upper): return np.ones_like(x) / (upper - lower) _, ax = plt.subplots(1, 3, figsize=(14, 3.5)) lower, upper = 0, 1 x = np.linspace(lower, upper) plot_uniform(uniform_pdf, x, lower, upper, pdf=True, ax=ax[0], color=\u0026#39;C0\u0026#39;) lower, upper = 0.5, 2.5 x = np.linspace(lower, upper) plot_uniform(uniform_pdf, x, lower, upper, pdf=True, ax=ax[1], color=\u0026#39;C1\u0026#39;) lower, upper = 2, 2.5 x = np.linspace(lower, upper) plot_uniform(uniform_pdf, x, lower, upper, pdf=True, ax=ax[2], color=\u0026#39;C2\u0026#39;) plt.tight_layout() _, ax = plt.subplots(figsize=(6, 4)) for i, (lower, upper) in enumerate(zip((0, 0.5, 2), (1, 2.5, 2.5))): x = np.linspace(lower, upper) y = uniform_pdf(x, lower, upper) ax.plot(x, y, color=f\u0026#39;C{i}\u0026#39;, label=f\u0026#39;{lower=}, {upper=}\u0026#39;) ax.fill_between(x, y, color=f\u0026#39;C{i}\u0026#39;, alpha=.2) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_title(\u0026#39;$uniform_{pdf}(x; lower, upper)$\u0026#39;) ax.legend(fontsize=14); Let\u0026rsquo;s now turn to the mightier gamma distribution.\nThe gamma distribution #  The gamma distribution is based on this funny looking function, with two parameters:\n\\[ gamma(x; a) = x^{a-1}e^{-x} \\]\nWe will consider the cases where \\(x \u0026gt; 0\\) and \\(a \u0026gt; 0\\). As with the uniform, \\(x\\) represents the possible random outcomes, while \\(a\\), analogous to the \\(lower\\) and \\(upper\\), is a fixed parameter that characterizes its probability density. Unlike the uniform, this distribution is not bounded, it allows for infinitely large \\(x\\) outcomes.\nLooking at the expression, one can notice that the first part \\(x^{a-1}\\) will grow quickly as x goes to \\(\\infty\\), while the second part \\(e^{-x}\\) will decrease exponentially as \\(x\\) increases. By multiplying the two we will obtain a rising curve followed by a declining one, as the two terms change in relative importance. Here is an example with \\(a=4\\)\na = 4 x = np.linspace(0, 20, 100) _, ax = plt.subplots(1, 3, figsize=(14, 3.5)) y1 = x ** (a-1) ax[0].plot(x, y1, lw=3, color=\u0026#39;k\u0026#39;) ax[0].set_title(\u0026#39;$x^{3}$\u0026#39;) y2 = np.exp(-x) ax[1].plot(x, y2, lw=3, color=\u0026#39;k\u0026#39;) ax[1].set_title(\u0026#39;$e^{-x}$\u0026#39;) y3 = y1 * y2 ax[2].plot(x, y3, lw=3, color=\u0026#39;k\u0026#39;) ax[2].set_title(\u0026#39;$x^{3}e^{-x}$\u0026#39;) plt.tight_layout() Let\u0026rsquo;s turn it into a python function and see how the function changes with different \\(a\\).\ndef gamma(x, a): return x**(a-1)*np.exp(-x) def plot_gamma(f, x, a, ax, color, pdf=False): y = f(x, a) # Integrate area between 0 and +inf area_y = scipy.integrate.quad(f, 0, np.inf, args=(a,))[0] ax.plot(x, y, color=color) ax.fill_between(x, y, color=color, alpha=.2) ax.text(0.5, 0.5, f\u0026#39;area={area_y:.1f}\u0026#39;, transform=ax.transAxes, ha=\u0026#39;center\u0026#39;, va=\u0026#39;center\u0026#39;) ax.set_xlabel(\u0026#39;x\u0026#39;) if not pdf: ax.set_title(f\u0026#39;$gamma(x; {a})$\u0026#39;) else: ax.set_title(f\u0026#39;$gamma_{{pdf}}(x; {a})$\u0026#39;) _, ax = plt.subplots(1, 3, figsize=(14, 3.5)) x = np.linspace(0, 20, 1000) plot_gamma(gamma, x, 4, ax=ax[0], color=\u0026#39;C0\u0026#39;) plot_gamma(gamma, x, 6, ax=ax[1], color=\u0026#39;C1\u0026#39;) plot_gamma(gamma, x, 8, ax=ax[2], color=\u0026#39;C2\u0026#39;) plt.tight_layout() Okay, perhaps not the most amazing looking curve. But we are not looking for amazing, just nice! And there are a couple of nice things going on:\n The function seems to be positive everywhere in the range \\(x \\subset [0, \\infty]\\) It looks like the graph does not blow up as \\(x\\) goes to \\(\\infty\\), even for larger \\(a\\) The \\(a\\) parameter seems to have a meaningful effect on the shape of the function.  Points 1 and 2 are needed for defining an unbounded positive probability density function, while point 3 is nice for making it a useful distribution.\nIf you never heard about the gamma function, you might be surprised to find out that the areas for the three examples all evaluate to integer values (subject to float representation). Even more surprising is the fact that these values correspond to \\((a-1)!\\). And analogous to the factorial function, the area under this expression preserves a certain recursive form of \\(n! = n(n-1)!\\), for any positive numbers (while the factorial is only defined for integers). This surprising fact was discovered by Euler in 1730.\nnp.math.factorial(3), np.math.factorial(5), np.math.factorial(7) (6, 120, 5040)  scipy.special.gamma(3.5+1), 3.5 * scipy.special.gamma(3.5) (11.63172839656745, 11.63172839656745)   Finding the normalization constant #  In order to obtain a valid density function, we must be able to scale this function so that it integrates to 1. In theory, all we need to do is divide the original expression by the total area, as we did for the uniform.\n\\[ gamma_{pdf}(x; a) = \\frac{x^{a-1}e^{-x}}{\\int_{0}^{\\infty}{x^{a-1} e^{-x} dx}} \\]\nHowever, unlike the uniform, there is no simple form for the integral in the denominator. That might be slightly annoying, but it\u0026rsquo;s not critical as long as we can compute it. In the code used to generate the plots above, I used a generic integration approximator, but, fortunately for us, some folks have found more reliable or accurate approximations that can be used to compute this integral (see the Wikipedia entries for Stirling\u0026rsquo;s and Lanczos\u0026rsquo;s approximations).\nFinally, just because we cannot simplify the denominator expression, it doesn\u0026rsquo;t mean we have to write it everywhere. Let\u0026rsquo;s do some mathematical refactoring by encapsulating the denominator inside a function. To keep it math-appropriate we will use a Greek letter for its name, although something more verbose such as total_area_gamma would work as well. To keep with the convention, we will give it the uppercase gamma symbol \\(\\Gamma\\):\n\\[ gamma_{pdf}(x; a) = \\frac{x^{a-1}e^{-x}}{\\Gamma(a)} \\]\nScipy provides an implementation of the gamma function via scipy.special.gamma.  Time to plot our proper gamma density function:\ndef gamma_pdf(x, a): return x**(a-1)*np.exp(-x) / scipy.special.gamma(a) _, ax = plt.subplots(1, 3, figsize=(14, 3.5)) x = np.linspace(0, 20, 1000) plot_gamma(gamma_pdf, x, 4, pdf=True, ax=ax[0], color=\u0026#39;C0\u0026#39;) plot_gamma(gamma_pdf, x, 6, pdf=True, ax=ax[1], color=\u0026#39;C1\u0026#39;) plot_gamma(gamma_pdf, x, 8, pdf=True, ax=ax[2], color=\u0026#39;C2\u0026#39;) plt.tight_layout() _, ax = plt.subplots(figsize=(6, 4)) x = np.linspace(0, 20, 1000) for i, a in enumerate((4, 6, 8)): y = gamma_pdf(x, a) ax.plot(x, y, color=f\u0026#39;C{i}\u0026#39;, label=f\u0026#39;{a=}\u0026#39;) ax.fill_between(x, y, color=f\u0026#39;C{i}\u0026#39;, alpha=.2) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_title(\u0026#39;gamma_pdf(x; a)\u0026#39;) ax.legend(); Adding a scale parameter #  Our pdf is neat, but can we do something more with it? Most distributions have two parameters, and we got only one: \\(a\\). Since more flexibility can always come in handy, why not add another one? What about adding a scaling \\(b\\) parameter?\nAccording to Wikipedia, this is pretty straightforward once we have a valid pdf, which we already do:\n\\[ scaled_{pdf}(x, b) = \\frac{1}{b}standard_{pdf}(\\frac{x}{b}) \\]\nIn our case\n\\[ gamma_{pdf}(x; a, b) = \\frac{(\\frac{x}{b})^{a-1} e^{-(\\frac{x}{b})}}{b \\Gamma(a)} \\]\nWhich we can arrange as\n\\[ gamma_{pdf}(x; a, b) = \\frac{x^{a-1} e^{-(\\frac{x}{b})}}{b^{a-1}b \\Gamma(a)} = \\frac{x^{a-1} e^{-(\\frac{x}{b})}}{b^{a} \\Gamma(a)} \\]\nOr alternatively, we can use a precision variable \\(c = \\frac{1}{b}\\)\n\\[ gamma_{pdf}(x; a, c) = \\frac{c(xc)^{a-1} e^{-(xc)}}{\\Gamma(a)} = \\frac{c^ax^{a-1} e^{-xc}}{\\Gamma(a)} \\]\nLet\u0026rsquo;s see what we can do with this extra parameter:\ndef gamma_pdf(x, a, c=1): return c**a * x**(a-1) * np.exp(-x*c) / scipy.special.gamma(a) def plot_gamma_variations(a, ax, color): x = np.linspace(0, 20, 1000) base_color = seaborn.saturate(color) for c, saturation in zip((2, 1, 0.5), (0.8, 0.4, 0.2)): y = gamma_pdf(x, a=a, c=c) color = seaborn.desaturate(base_color, saturation) ax.plot(x, y, color=color, label=f\u0026#39;{c=}\u0026#39;) ax.fill_between(x, y, color=color, alpha=.2) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_title(f\u0026#39;$gamma_{{pdf}}(x;{a},c)$\u0026#39;) ax.legend() _, ax = plt.subplots(1, 3, figsize=(14, 3.5)) plot_gamma_variations(a=4, ax=ax[0], color=\u0026#39;C0\u0026#39;) plot_gamma_variations(a=6, ax=ax[1], color=\u0026#39;C1\u0026#39;) plot_gamma_variations(a=8, ax=ax[2], color=\u0026#39;C2\u0026#39;) plt.tight_layout() The parameter names \\(a\\), \\(b\\), and \\(c\\), deviate from traditional characterizations of the gamma distribution. Wikipedia, for example, uses \\(k\\) and \\(\\theta\\) for our \\(a\\) and scale \\(b\\), and a distinct pair of parameter names \\(\\alpha\\) and \\(\\beta\\) for our \\(a\\) and precision \\(c\\) parameterization. \\(\\lambda\\) is also commonly used for the precision parameter, as this is linked conceptually to the exponential \\(\\lambda\\) rate parameter. Speaking of which, \u0026ldquo;rate\u0026rdquo; is an alternative, and perhaps more common name, for \u0026ldquo;precision\u0026rdquo;.  Bonus: deriving the CDF of the gamma distribution #  We got the gamma pdf, let\u0026rsquo;s get the cumulative distribution function (CDF) as well. Recall that the CDF of a distribution \\(X\\) is the matematical function that returns the \\(P(X\u0026lt;=x)\\), that is the probability that the random \\(x\\) value is lower or equal to a specific number. Since it defines an interval, in our case \\([0, x]\\), it is appropriate to talk about probabilities and not just densities.\n\\[ gamma\\_cdf(x; a, c) = \\frac{\\int_0^x c^a(u)^{a-1} e^{-(cu)} du}{\\Gamma(a)}\\]\nNote that we need a new variable \\(u\\) to distinguish the limit of integration from the integrand. Also, for the very same reason as before, the \\(\\Gamma(a)\\) is there to make sure the CDF evaluates to \\(1\\) when \\(x=\\infty\\)\nThis article would have been slightly less unrealistic if it had started with the derivation of the CDF, since the Gamma integral function is what actually came down to us through history.  def plot_gamma_integral(a, ax, color, cdf=False): x = np.logspace(0.1, 1.3) y = scipy.special.gammainc(a, x) assymptote = 1 # gammainc returns the already normalized incomplete integral gamma = scipy.special.gamma(a) y *= gamma ax.plot(x, y, color=color) ax.fill_between(x, y, color=color, alpha=.2) ax.set_yticks([gamma]) ax.grid(None, axis=\u0026#39;x\u0026#39;) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_title(f\u0026#39;$\\int_0^{{x}}gamma(u; {a}, 1) du$\u0026#39;) _, ax = plt.subplots(1, 3, figsize=(14, 3.5)) plot_gamma_integral(4, ax=ax[0], color=\u0026#39;C0\u0026#39;) plot_gamma_integral(6, ax=ax[1], color=\u0026#39;C1\u0026#39;) plot_gamma_integral(8, ax=ax[2], color=\u0026#39;C2\u0026#39;) plt.tight_layout() _, ax = plt.subplots(figsize=(6, 4)) x = np.logspace(0.1, 1.3) for i, a in enumerate((4, 6, 8)): y = scipy.special.gammainc(a, x) ax.plot(x, y, color=f\u0026#39;C{i}\u0026#39;, label=f\u0026#39;{a=}\u0026#39;) ax.fill_between(x, y, color=f\u0026#39;C{i}\u0026#39;, alpha=.2) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_title(f\u0026#39;$gamma_{{cdf}}(x; a, 1)$\u0026#39;) ax.legend(); The CDF expression can be further simplified with a change of variables \\(t = cu\\) \\[ \\begin{aligned} gamma\\_cdf(x; a, c) \u0026amp;= \\frac{\\int_0^x c^a(u)^{a-1} e^{-(cu)} du}{\\Gamma(a)} \\\\ \u0026amp;= \\frac{1}{\\Gamma(a)} \\int_0^x c(cu)^{a-1} e^{-(cu)} du \\\\ \u0026amp;= \\frac{1}{\\Gamma(a)} \\int_0^{xc} c(t)^{a-1} e^{-t} \\frac{1}{c}dt \\quad \\begin{cases} \\text{change of variables} \\\\ t = cu \\\\ dt = c\\,du \\\\ \\frac{1}{c}dt = du \\\\ \\end{cases} \\\\ \u0026amp;= \\frac{1}{\\Gamma(a)} \\int_0^{xc} t^{a-1} e^{-t} dt \\end{aligned} \\]\nWhich reveals that the inverse scaling by \\(c\\) has an effect on increasing or reducing the area of a standard gamma distribution that is contained below \\(x\\)\n_, ax = plt.subplots(figsize=(6, 4)) x = np.logspace(0.1, 1.6, 200) base_color = seaborn.saturate(\u0026#39;C0\u0026#39;) cutoff = 5 for c, saturation in zip( (1, 0.5, 0.25), (0.8, 0.4, 0.2) ): color = seaborn.desaturate(base_color, saturation) where = x\u0026lt;=(cutoff/c) y = scipy.special.gammainc(4, x*c) ax.plot(x[where], y[where], color=color, label=f\u0026#39;{c=}\u0026#39;) ax.plot(x[~where], y[~where], color=color, ls=\u0026#39;--\u0026#39;) ax.fill_between(x, y, where=where, color=color, alpha=.2) ax.set_xticks([5, 10, 20]) ax.set_yticks([0, scipy.special.gammainc(4, cutoff), 1]) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_title(f\u0026#39;$gamma_{{cdf}}(x; 4, c)$\u0026#39;) # ax.set_ylabel(r\u0026#39;$\\frac{\\int_0^{xc}{f(a, u)} du}{area\\_f(x)}$\u0026#39;) ax.legend(); Similar to what we did with the pdf, it\u0026rsquo;s common to refactor the numerator integral expression into its own function. This is called lower incomplete gamma function, or more succintly the lower case gamma \\(\\gamma\\) letter. \\[ gamma\\_cdf(x; a, c) = \\frac{\\int_0^{xc} t^{a-1} e^{-t} dt}{\\Gamma(a)} = \\frac{\\gamma(a, xc)}{\\Gamma(a)} \\]\nAs with the gamma function, there are quick and stable numerical implementations of the incomplete gamma function. In scipy this can be obtained by scipy.special.gammainc(a, x) * scipy.special.gamma(a). The multiplication is needed because the default scipy.special.gammainc already contains the \\(gamma(a)\\) normalization term.  Similarly to the gamma distribution, which gets its name from the gamma integral function \\(\\Gamma\\), the beta distribution also gets its name from the beta integral function \\(B\\). In fact the two distributions are closely related.  nb2hugo ~/Documents/ricardoV94.github.io.website/raw_notebooks/gamma_distribution.ipynb --site-dir ~/Documents/ricardoV94.github.io.website/ --section posts  "},{"id":1,"href":"/posts/hello-world/","title":"Hello, World","section":"Blog","content":"Introduction #  Not much to see here\n"},{"id":2,"href":"/topics/gsoc-2021/proposal/","title":"Proposal","section":"Gsoc 2021","content":"PyMC3: Make SMC-ABC faster and more flexible #  Intro #  PyMC3 provides state-of-the-art tools to specify rich mathematical probabilistic models and algorithms to efficiently approximate the posterior distribution of such models conditioned on observed data. One such algorithm is the Sequential Monte Carlo (SMC) sampler which is capable of drawing samples from complex posterior distributions (e.g., multimodal distributions).\nIn addition to traditional Bayesian inference, SMC can also be used to perform Approximate Bayesian Computation (ABC), which allows one to define models without a pure mathematical likelihood term, which is difficult to derive in many complex real world problems. To achieve this, SMC-ABC makes use of a “Simulator” function that is capable of returning simulated observed data given different unobserved parameters.\nThis project seeks to extend the documentation, performance and flexibility of SMC and SMC-ABC sampling in PyMC3, to make it competitive with specialized libraries while remaining accessible to the large user-base of the PyMC3 library.\nTechnical Details #  This project involves making improvements to the SMC(-ABC) sampler in the light of the recent changes to the Aesara backend in the upcoming V4 release. This ongoing transition involves a large change to the library codebase and it is likely that some features of SMC(-ABC) may be broken in the process.\nPhase 1 #  Weeks 1 - 3 (June 7th - June 27th)\nAs such, the first phase of the project will focus on fixing any potential regressions, while aligning the code logic to be more in line with the core library after the V4 transition. Some prior issues that were detected through personal experimentation and user discussions will also be tackled during this initial phase:\n Wrap the pm.Simulator object around RandomOps for a more consistent integration within the PyMC3 model object Fix currently broken Prior and Posterior predictive sampling when using pm.Simulator Fix currently broken graphviz representation when using Pm.Simulator Allow pm.Simulator wrapped function to take non-named inputs and provide more useful error messages for improper initialization (see discourse issue) Add progress bar in each beta stage for better visual feedback of sampling speed during SMC Automatically select SMC-ABC when pm.Simulator is present in a Model object Fix missing documentation for pm.Simulator Test possibility for improved performance via vectorized evaluation of the graph logp across SMC particles Test parallel sampling in SMC and provide that as default.  Phase 2 #  Weeks 4-6 (June 28th - July 18th)\nIn the second phase of this project, the core SMC-ABC functions will be refactored to increase its modularity and facilitate the implementation and testing of different algorithms going into the future. During this time I also plan to write 3 in-depth PyMC-examples that demonstrate the use of SMC(-ABC) in PyMC3. These include:\n Definition of a consistent API based on python functions with standard I/O that can be plugged in at the different stages of SMC: initialize_population, mutate, tune, etc\u0026hellip; for customization of the sampling algorithm. Provide two standard SMC algorithms: Independent MvNormal (already implemented in v3), and Normal random walk (reintroduced for benchmarking and assessing modularity of the SMC-ABC functionality) Add a Probability Density Approximation (PDA) method based on a Gaussian or Epanechnikov kernel as an alternative to the epsilon based distance Pseudo-likelihood (Turner \u0026amp; Sederberg, 2014). Documentation for all changes Write 3 new PyMC-examples that illustrate SMC(-ABC) features:  Adapt and extend toy example from Bååth (2014) which emphasizes the advantages and disadvantages of ABC sampling (need to derive complex likelihoods vs speed / accuracy). Contrast the accuracy of ABC vs true-likelihood sampling in simple Cognitive research models with known Likelihood forms described in Palestro et al., (2018). Tutorial on how to implement a custom SMC-kernel (Independent Metropolis Hastings with mixture of gaussians) to facilitate future research and development of SMC(-ABC) in PyMC3.    Phase 3 #  Weeks 7-10 (July 19th - August 15th)\nThe third and final phase of the project will explore promising extensions of SMC(-ABC) beyond what is currently offered in the PyMC3 library. Namely:\n Proof of concept SMC sampling with NUTS kernel for updating of particles. Combination of Simulator with normal NUTS blocked sampling by updating first order parameters with the ABC Pseudolikelihood, and hyperparameters that do not feed directly into the (Pseudo-)Likelihood with NUTS (a.k.a, Hierarchical Gibbs Sampling, see Turner \u0026amp; van Zandt, 2014) Allow for multiple Simulators within a single model (e.g., in a hierarchical model where one might want to use different epsilon levels for different observations/ users) Allow the Mixing of Simulator Pseudo-likelihoods and True-likelihoods in a single model.  Community Bonding Period #  (May 17 - June 7)\nI plan to make use of the Community Bonding Period to further refine the goals of my project with the supervisors and core-developer team. During this time I plan to reach out to the PyMC3 user community (via https://discourse.pymc.io/) to elicit feedback and suggestions on the proposal. Finally, during this phase I will launch a monthly code-sharing “contest” where users can submit and review PyMC3 models that highlight interesting library features as well as general statistical and theoretical ideas that arise in the practice of bayesian modelling and inference.\nWhy Me? #  I am a PhD Student in the field of Cognitive Science, and enthusiast self-taught statistician. I am interested in the features of Bayesian models not only as a useful statistical tool for research data analysis but also as a model of human and animal cognition. In addition, I am a fan of the Python language and I have been learning, using it and teaching it for 6 years. Development Experience\nI have been an active contributor to the PyMC3 library since last December, having used the library extensively in research for nearly 2 years now. I became an official core-developer around January 2021, and I have engaged with the project and community extensively since and prior to that. A few of my (merged) PRs are listed below:\nNew functionalities:\n #4419: Add informative user Warning when doing prior/posterior predictive sampling in models with arbitrary Potentials #4407: Increase numerical stability of ExGaussian logp and logcdf methods #4387: Implement logcdf methods for discrete distributions #4373: Complete stale DirichletMultinomial distribution PR #4360: Improve math.logsumexp to work with infinite values #4298: Make jitter during initializations of NUTS more robust #4134: Add parameterizations to NegativeBinomial in terms of n and p  Testing:\n #4461: Unseed wrongly seeded tests #4448: Add tests for difficult to debug bugs #4393: Make logcdf tests more exhaustive  Minor bugfixes:\n #4366 #4211  Why PyMC3? #  PyMC3 provides easy access to state-of-the-art tools to perform Bayesian inference. I have used this library extensively during my research and teaching, and found it invaluable within the Python (and Bayesian) ecosystem. It’s simple syntax, extensive documentation and large active community make it easy for beginners to transition into Bayesian analysis, which is increasingly viewed as a more principled and more fool-proof way of performing statistical analysis and informed decision making in research and industry settings.\nReferences #   Bååth, R (2014). Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman. http://www.sumsar.net/blog/2014/10/tiny-data-and-the-socks-of-karl-broman/ Palestro, J. J., Sederberg, P. B., Osth, A. F., Van Zandt, T., \u0026amp; Turner, B. M. (2018). Likelihood-free methods for cognitive science. Springer International Publishing. Turner, B. M., \u0026amp; Sederberg, P. B. (2014). A generalized, likelihood-free method for posterior estimation. Psychonomic bulletin \u0026amp; review, 21(2), 227-250. Turner, B. M., \u0026amp; Van Zandt, T. (2014). Hierarchical approximate Bayesian computation. Psychometrika, 79(2), 185-209.  "}]