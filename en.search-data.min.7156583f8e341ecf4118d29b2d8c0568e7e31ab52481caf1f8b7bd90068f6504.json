[{"id":0,"href":"/posts/hello-world/","title":"Hello, World","section":"Blog","content":"Introduction #  Not much to see here\n"},{"id":1,"href":"/topics/gsoc-2021/proposal/","title":"Proposal","section":"Gsoc 2021","content":"PyMC3: Make SMC-ABC faster and more flexible #  Intro #  PyMC3 provides state-of-the-art tools to specify rich mathematical probabilistic models and algorithms to efficiently approximate the posterior distribution of such models conditioned on observed data. One such algorithm is the Sequential Monte Carlo (SMC) sampler which is capable of drawing samples from complex posterior distributions (e.g., multimodal distributions).\nIn addition to traditional Bayesian inference, SMC can also be used to perform Approximate Bayesian Computation (ABC), which allows one to define models without a pure mathematical likelihood term, which is difficult to derive in many complex real world problems. To achieve this, SMC-ABC makes use of a “Simulator” function that is capable of returning simulated observed data given different unobserved parameters.\nThis project seeks to extend the documentation, performance and flexibility of SMC and SMC-ABC sampling in PyMC3, to make it competitive with specialized libraries while remaining accessible to the large user-base of the PyMC3 library.\nTechnical Details #  This project involves making improvements to the SMC(-ABC) sampler in the light of the recent changes to the Aesara backend in the upcoming V4 release. This ongoing transition involves a large change to the library codebase and it is likely that some features of SMC(-ABC) may be broken in the process.\nPhase 1 #  Weeks 1 - 3 (June 7th - June 27th)\nAs such, the first phase of the project will focus on fixing any potential regressions, while aligning the code logic to be more in line with the core library after the V4 transition. Some prior issues that were detected through personal experimentation and user discussions will also be tackled during this initial phase: Wrap the pm.Simulator object around RandomOps for a more consistent integration within the PyMC3 model object Fix currently broken Prior and Posterior predictive sampling when using pm.Simulator Fix currently broken graphviz representation when using Pm.Simulator Allow pm.Simulator wrapped function to take non-named inputs and provide more useful error messages for improper initialization (see discourse issue) Add progress bar in each beta stage for better visual feedback of sampling speed during SMC Automatically select SMC-ABC when pm.Simulator is present in a Model object Fix missing documentation for pm.Simulator Test possibility for improved performance via vectorized evaluation of the graph logp across SMC particles Test parallel sampling in SMC and provide that as default.\nPhase 2 #  Weeks 4-6 (June 28th - July 18th)\nIn the second phase of this project, the core SMC-ABC functions will be refactored to increase its modularity and facilitate the implementation and testing of different algorithms going into the future. During this time I also plan to write 3 in-depth PyMC-examples that demonstrate the use of SMC(-ABC) in PyMC3. These include: Definition of a consistent API based on python functions with standard I/O that can be plugged in at the different stages of SMC: initialize_population, mutate, tune, etc\u0026hellip; for customization of the sampling algorithm. Provide two standard SMC algorithms: Independent MvNormal (already implemented in v3), and Normal random walk (reintroduced for benchmarking and assessing modularity of the SMC-ABC functionality) Add a Probability Density Approximation (PDA) method based on a Gaussian or Epanechnikov kernel as an alternative to the epsilon based distance Pseudo-likelihood (Turner \u0026amp; Sederberg, 2014). Documentation for all changes Write 3 new PyMC-examples that illustrate SMC(-ABC) features: Adapt and extend toy example from Bååth (2014) which emphasizes the advantages and disadvantages of ABC sampling (need to derive complex likelihoods vs speed / accuracy). Contrast the accuracy of ABC vs true-likelihood sampling in simple Cognitive research models with known Likelihood forms described in Palestro et al., (2018). Tutorial on how to implement a custom SMC-kernel (Independent Metropolis Hastings with mixture of gaussians) to facilitate future research and development of SMC(-ABC) in PyMC3.\nPhase 3 #  Weeks 7-10 (July 19th - August 15th)\nThe third and final phase of the project will explore promising extensions of SMC(-ABC) beyond what is currently offered in the PyMC3 library. Namely: Proof of concept SMC sampling with NUTS kernel for updating of particles. Combination of Simulator with normal NUTS blocked sampling by updating first order parameters with the ABC Pseudolikelihood, and hyperparameters that do not feed directly into the (Pseudo-)Likelihood with NUTS (a.k.a, Hierarchical Gibbs Sampling, see Turner \u0026amp; van Zandt, 2014) Allow for multiple Simulators within a single model (e.g., in a hierarchical model where one might want to use different epsilon levels for different observations/ users) Allow the Mixing of Simulator Pseudo-likelihoods and True-likelihoods in a single model.\nCommunity Bonding Period #  (May 17 - June 7)\nI plan to make use of the Community Bonding Period to further refine the goals of my project with the supervisors and core-developer team. During this time I plan to reach out to the PyMC3 user community (via https://discourse.pymc.io/) to elicit feedback and suggestions on the proposal. Finally, during this phase I will launch a monthly code-sharing “contest” where users can submit and review PyMC3 models that highlight interesting library features as well as general statistical and theoretical ideas that arise in the practice of bayesian modelling and inference.\nWhy Me? #  I am a PhD Student in the field of Cognitive Science, and enthusiast self-taught statistician. I am interested in the features of Bayesian models not only as a useful statistical tool for research data analysis but also as a model of human and animal cognition. In addition, I am a fan of the Python language and I have been learning, using it and teaching it for 6 years. Development Experience\nI have been an active contributor to the PyMC3 library since last December, having used the library extensively in research for nearly 2 years now. I became an official core-developer around January 2021, and I have engaged with the project and community extensively since and prior to that. A few of my (merged) PRs are listed below:\nNew functionalities:\n #4419: Add informative user Warning when doing prior/posterior predictive sampling in models with arbitrary Potentials #4407: Increase numerical stability of ExGaussian logp and logcdf methods #4387: Implement logcdf methods for discrete distributions #4373: Complete stale DirichletMultinomial distribution PR #4360: Improve math.logsumexp to work with infinite values #4298: Make jitter during initializations of NUTS more robust #4134: Add parameterizations to NegativeBinomial in terms of n and p  Testing:\n #4461: Unseed wrongly seeded tests #4448: Add tests for difficult to debug bugs #4393: Make logcdf tests more exhaustive  Minor bugfixes:\n #4366 #4211  Why PyMC3? #  PyMC3 provides easy access to state-of-the-art tools to perform Bayesian inference. I have used this library extensively during my research and teaching, and found it invaluable within the Python (and Bayesian) ecosystem. It’s simple syntax, extensive documentation and large active community make it easy for beginners to transition into Bayesian analysis, which is increasingly viewed as a more principled and more fool-proof way of performing statistical analysis and informed decision making in research and industry settings.\nReferences #   Bååth, R (2014). Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman. http://www.sumsar.net/blog/2014/10/tiny-data-and-the-socks-of-karl-broman/ Palestro, J. J., Sederberg, P. B., Osth, A. F., Van Zandt, T., \u0026amp; Turner, B. M. (2018). Likelihood-free methods for cognitive science. Springer International Publishing. Turner, B. M., \u0026amp; Sederberg, P. B. (2014). A generalized, likelihood-free method for posterior estimation. Psychonomic bulletin \u0026amp; review, 21(2), 227-250. Turner, B. M., \u0026amp; Van Zandt, T. (2014). Hierarchical approximate Bayesian computation. Psychometrika, 79(2), 185-209.  "}]