<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on As long as everything adds up to one</title><link>https://ricardov94.github.io/</link><description>Recent content in Introduction on As long as everything adds up to one</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 13 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://ricardov94.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Successive Wins, from Fifty Challenging Problems in Probability</title><link>https://ricardov94.github.io/posts/successive_wins/</link><pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate><guid>https://ricardov94.github.io/posts/successive_wins/</guid><description>The author poses the following problem:
To encourage Elmer’s promising tennis career, his father offers him a prize if he wins (at least) two tennis sets in a row in a three-set series to be played with his father and the club champion alternately: father-champion-father (FCF) or champion-father-champion (CFC), according to Elmer’s choice. The champion is a better player than Elmer’s father. Which series should Elmer choose?
The answer is CFC, and the author emphasizes, after listing all the possible sequences, the importance of the middle match where a victory must absolutely be scored.</description></item><item><title>Praising learning accidents</title><link>https://ricardov94.github.io/posts/learning_accidents/</link><pubDate>Sun, 08 Oct 2023 00:00:00 +0000</pubDate><guid>https://ricardov94.github.io/posts/learning_accidents/</guid><description>I have always been an unremarkable football player. I occasionally joined some informal games in the breaks between classes. More when I was younger, and less often as I grew older. By middle school (grades 7-9) I barely played. I was much more successful at miniature sports: ping pong or table soccer.
However I remember this one afternoon in 8th grade, when I joined a game by the insistence of my classmates.</description></item><item><title>Derivatives, integrals, anti-derivatives and anti-integrals</title><link>https://ricardov94.github.io/posts/integration_differentiation/</link><pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate><guid>https://ricardov94.github.io/posts/integration_differentiation/</guid><description>import numpy as np import matplotlib.pyplot as plt import seaborn seaborn.set_style(&amp;#39;darkgrid&amp;#39;) seaborn.set(font_scale=1.2) def draw_arrows(ax, top_from, top_to, top_text_xy, bottom_from, bottom_to, bottom_text_xy, diff_top=True): text_top = &amp;#34;Differentiation&amp;#34; text_bottom = &amp;#34;Integration&amp;#34; if not diff_top: text_top, text_bottom = text_bottom, text_top ax.annotate( &amp;#34;&amp;#34;, top_from, top_to, xycoords=&amp;#34;axes fraction&amp;#34;, size=&amp;#34;large&amp;#34;, ha=&amp;#34;left&amp;#34;, va=&amp;#34;center&amp;#34;, arrowprops=dict( arrowstyle=&amp;#34;-&amp;gt;&amp;#34;, color=&amp;#34;k&amp;#34;, lw=2, connectionstyle=&amp;#34;angle3,angleA=50,angleB=-50&amp;#34;, ), ) ax.text(*top_text_xy, text_top, transform=ax.transAxes, ha=&amp;#34;center&amp;#34;) ax.annotate( &amp;#34;&amp;#34;, bottom_from, bottom_to, xycoords=&amp;#34;axes fraction&amp;#34;, size=&amp;#34;large&amp;#34;, ha=&amp;#34;left&amp;#34;, va=&amp;#34;center&amp;#34;, arrowprops=dict( arrowstyle=&amp;#34;-&amp;gt;&amp;#34;, color=&amp;#34;k&amp;#34;, lw=2, connectionstyle=&amp;#34;angle3,angleA=50,angleB=-50&amp;#34;, ), ) ax.</description></item><item><title>A simple Sequential Monte Carlo algorithm for posterior approximation</title><link>https://ricardov94.github.io/posts/smc_basic_algorithm/</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://ricardov94.github.io/posts/smc_basic_algorithm/</guid><description>Introduction # Sequential Monte Carlo (SMC) can be used to take samples from posterior distributions, as an alternative to popular methods such as Markov Chain Monte Carlo (MCMC) like Metropolis-Hastings, Gibbs Sampling or more state-of-the-art Hamiltonian Monte Carlo (HMC) and No-U-Turn Sampler (NUTS).
Instead of creating a single Markov Chain, SMC works by keeping track of a large population of samples, which, in a similar spirit to Genetic Algorithms can be &amp;ldquo;selected&amp;rdquo; and &amp;ldquo;mutated&amp;rdquo; to evolve a better &amp;ldquo;fit&amp;rdquo; to the posterior distribution.</description></item><item><title>A most unprincipled derivation of the gamma distribution</title><link>https://ricardov94.github.io/posts/gamma_distribution/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://ricardov94.github.io/posts/gamma_distribution/</guid><description>Introduction # In this article I will derive the gamma distribution in a most unprincipled way.
Why? First, there are many good resources out there explaining how to derive the gamma distribution from first principles, usually involving these idealized things called poisson processes. These are great sources and I would definitely recommend them. This one by Aerin Kim is amazing.
However, more often than not, people use gamma distributions in real world problems for much more mundane reasons: It&amp;rsquo;s a well behaved yet flexible positive continuous distribution.</description></item></channel></rss>